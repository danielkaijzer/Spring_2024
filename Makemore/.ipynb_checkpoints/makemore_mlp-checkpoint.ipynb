{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "772f530d-001b-4052-b935-6c81d3abebfa",
   "metadata": {},
   "source": [
    "## Problem: \n",
    "    we want to solve: How to take larger contexts in a manageable way. \n",
    "    We cannot just use tables like we did with bigrams (context of one char to predict next char) because they grow exponentially. \n",
    "## Solution:\n",
    "    We will use approach used in Bengio et al. paper. We will embed chars into a n dimensional vector space (a space smaller than number of chars which is 27). \n",
    "    \n",
    "    In this space related chars will be clustered together within the vector space, allowing us to generalize relationships so we can predict next chars in novel starting points.\n",
    "    \n",
    "    At first the embeddings will be random, i.e., the vectors representing chars will be placed randomly within the vector space. Then, using backpropagation in our multilayer neural network, we will change the locations of the vectors within the vector space so that chars that are often combined (in a context of n characters) are placed closely together.\n",
    "    \n",
    "    \"We can, through the embedding space, transfer knowledge and generalize to novel scenarios.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d45aded-5db9-44d9-9f49-bec37b7e9a0b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### What our neural network architecture looks like:\n",
    "\n",
    "It will essentially consist of 3 main layers:\n",
    "1. An input layer\n",
    "    - where the size depends on: \n",
    "        1. n - the context length (how many chars we will use to predict next char)\n",
    "        2. m - the number of dimensions of the vector space we will use to embed chars (i.e., represent unique chars as a vector of m numbers).\n",
    "    - number of neurons in our input layer is n * m\n",
    "    - this first layer has linear neurons (i.e., neurons with no non-linearity) and their weight matrix is the LUT C (the embedding matrix). We are essentially encoding ints into one-hot, embedding them into the LUT, and then feeding the embedding LUT C into the next layer of our NN (which is 2. the hidden layer)\n",
    "2. A \"hidden layer\" that has a tanh non-linearity. This layer is fully connected to our input layer\n",
    "    - i.e., fully connected to the numbers representing the n chars used as inputs. So the n*m inputs will be fed into the tanh layer\n",
    "3. An output layer \n",
    "    - all neurons of output layer are fully connected to all neurons of tanh (non-linearity) layer\n",
    "    - will output a char chosen from the 27 possible number of chars. The chosen char will be the char with the highest probability based off of the previous n chars. And this probability will be tuned over several epochs to minimize a loss value (which measures the quality of our NN).\n",
    "        - Note: the loss will use negative log likelihood\n",
    "    - Most computation will be in this output layer. So this layer is the most performance expensive layer of our NN.\n",
    "    - This layer will have 27 logits, so this layer will have softmax layer on top of it. Everyone of these logits gets exponentiated and they are normalized so all logits sum to 1. This will then give us a useful probability distribution for the next char in the sequence.\n",
    "        - During training we will have labels (the identity of the correct next char in the sequence). That char (represented as an int index) will be used to pluck out the probability of that char. The NN will be trained, i.e., the parameters of our NN will be tuned, to maximize the probability of that char (given the n previous chars in the sequence). \n",
    "\n",
    "Note: These parameters of our NN are the weights and biases of the output layer and our hidden layer, as well as the embedding LUT (C) within our input layer. All these parameters are optimized using backprop.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f864b016-f0e2-4b39-a5dd-5b67b370e962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d9631da-2da8-4309-b72d-bd8bc57a8e84",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2ccdb0b-9e84-4da1-955b-1ef8c3def003",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words) # print number of names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8edd6f3b-d607-4aa4-950c-59bcb3940889",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "# build the vocab of chars and mappings to/from ints\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)} # chars from 1-26 are alpha\n",
    "stoi['.'] = 0 # special char to delineate begin/end of word\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b18c499-7285-41f7-9567-2901254ec446",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "... ----> e\n",
      "..e ----> m\n",
      ".em ----> m\n",
      "emm ----> a\n",
      "mma ----> .\n",
      "olivia\n",
      "... ----> o\n",
      "..o ----> l\n",
      ".ol ----> i\n",
      "oli ----> v\n",
      "liv ----> i\n",
      "ivi ----> a\n",
      "via ----> .\n",
      "ava\n",
      "... ----> a\n",
      "..a ----> v\n",
      ".av ----> a\n",
      "ava ----> .\n",
      "isabella\n",
      "... ----> i\n",
      "..i ----> s\n",
      ".is ----> a\n",
      "isa ----> b\n",
      "sab ----> e\n",
      "abe ----> l\n",
      "bel ----> l\n",
      "ell ----> a\n",
      "lla ----> .\n",
      "sophia\n",
      "... ----> s\n",
      "..s ----> o\n",
      ".so ----> p\n",
      "sop ----> h\n",
      "oph ----> i\n",
      "phi ----> a\n",
      "hia ----> .\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "\n",
    "block_size = 3 # context length; num of previous tokens (chars) used to predict next token (char)\n",
    "X, Y = [],[] # X contains inputs to NN, Y contains the labels for each example inside X\n",
    "\n",
    "for w in words[:5]: # loop through training set (or subset of training set)\n",
    "    \n",
    "    print(w) # print entire word (name)\n",
    "    context = [0] * block_size # padded tokens, remember that 0 maps to '.' (special char)\n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch] # enumerate chars; convert ch to int val\n",
    "        X.append(context) # build out array X which stores running context\n",
    "        Y.append(ix) # build out array Y for current character\n",
    "        print(''.join(itos[i] for i in context), '---->', itos[ix])\n",
    "        context = context[1:] + [ix] # crop and append (rolling window of context), decrement padding as chars are appended\n",
    "        # remember location 1 above refers to second char in string\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "456b7f2c-6fbf-4a27-b9ac-2f0f0587f4b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.int64, torch.Size([32]), torch.int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype\n",
    "# from the 5 words (inputs) we have created a data set of 32 examples\n",
    "# and each input to the NN is 3 integers \n",
    "# label Y is also an integer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "598135ab-d017-4604-a912-a3787d1784d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0],\n",
       "        [ 0,  0,  5],\n",
       "        [ 0,  5, 13],\n",
       "        [ 5, 13, 13],\n",
       "        [13, 13,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0, 15],\n",
       "        [ 0, 15, 12],\n",
       "        [15, 12,  9],\n",
       "        [12,  9, 22],\n",
       "        [ 9, 22,  9],\n",
       "        [22,  9,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  1],\n",
       "        [ 0,  1, 22],\n",
       "        [ 1, 22,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  9],\n",
       "        [ 0,  9, 19],\n",
       "        [ 9, 19,  1],\n",
       "        [19,  1,  2],\n",
       "        [ 1,  2,  5],\n",
       "        [ 2,  5, 12],\n",
       "        [ 5, 12, 12],\n",
       "        [12, 12,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0, 19],\n",
       "        [ 0, 19, 15],\n",
       "        [19, 15, 16],\n",
       "        [15, 16,  8],\n",
       "        [16,  8,  9],\n",
       "        [ 8,  9,  1]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X # X is a n x m matrix\n",
    "# n = number of input chars (examples) and m is context length (num of chars used to predict next char)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c810cd-d742-4165-91e8-1d3f9ccc34d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Task: Build NN that takes Xs and predicts Ys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00f9712-8f68-4fd6-827c-e9c228ff6d89",
   "metadata": {},
   "source": [
    "## How embedding chars into the Look Up Table works\n",
    "\n",
    "We will use a char (an int between 0-26) to index into a $27*m$ matrix that represents a LUT, where m is the number of dimensions of a vector space we want to embed chars into.\n",
    "\n",
    "Every index (representing a char) is plucking out a row of the embedding matrix, so that each index is converted into the m-dimensional vector that corresponds to the embedding vector for that char.\n",
    "\n",
    "So basically embedding means we will represent an int (which itself represents a char) as a vector of m numbers (assuming float values). So if we have a context length of 3 chars and m = 30. Our input layer will have 90 numbers that represent 3 chars.\n",
    "\n",
    "The same LUT matrix will be shared by all chars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a1ba86-7306-43ca-98b4-859c05786863",
   "metadata": {},
   "source": [
    "### Build lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbc79b5b-8ef4-4404-b717-321859294b1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# just like Bengio et al. paper, we will embed 27 possible chars\n",
    "# in smaller dimensional space\n",
    "\n",
    "# each of the 27 chars will have a 2-dimensional embedding\n",
    "C = torch.randn([27,2]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bb0bf7-0258-436b-af35-99eeca04ec51",
   "metadata": {},
   "source": [
    "#### We will embedd all ints inside input X into LUT C \n",
    "\n",
    "There are two ways to embed a single int (e.g., 5) into LUT C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9eb0f3-ad28-4dcb-8d88-ae5c686a71ee",
   "metadata": {},
   "source": [
    "#### 1) take C and index into row 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71bbb42e-ba88-4303-ae43-7de88b57157c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3085, 0.3569])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0909044e-f23f-40f8-b106-e5fa9413c6cb",
   "metadata": {},
   "source": [
    "##### Method 2: use one-hot encoding \n",
    "- Even though it may not immediately seem like it, this method is actually identical to first method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a16a793f-c4a3-458d-8433-c428569acffa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3085, 0.3569])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a 27-D vector (i.e., array of 27 nums) with all 0s except for the 5th element in vector which is 1\n",
    "# i.e., 5th dimension is 1\n",
    "# we then cast to float() so we can multiply by matrix C\n",
    "\n",
    "# the one-hot encoding will essentially pluck out 5th row of C\n",
    "# because 0s mask the other rows\n",
    "# giving us the same result as the first method for embeding an int above\n",
    "F.one_hot(torch.tensor(5), num_classes=27).float() @ C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1375406a-1574-4355-b0fd-d07d4902999f",
   "metadata": {},
   "source": [
    "#### For efficiency we will use first method for embedding (just indexing into row of LUT C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6469c9-9d19-41fe-9a79-6dda32906ab4",
   "metadata": {},
   "source": [
    "Even easier, We can embed using tensors to simultaneously embed all 32x3 input ints (in array X) into the LUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39ae6d71-98fc-4cc5-93e4-934f2a42ad12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# C[X] # prints the embedding of inputs tensor X\n",
    "# C[X].shape # [32,3,2] # 32 input nums, context length 3, embedded into 2 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f3cdc82-253f-42ad-9854-eb189732bb69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[13,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "926173ba-8d40-4dbc-9071-d079e2eb8479",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.4563, 0.8970])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[X][13,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01ac5411-d57e-4ad4-a722-9d0738aac3d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.4563, 0.8970])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebe6f9b4-2786-429f-b645-03618f587057",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X]\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcdcba7-fb03-40b3-8990-71291992c84a",
   "metadata": {},
   "source": [
    "## Construct hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3ee453b-1731-4855-b847-23c131cea8fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# weights, the num of inputs to this layer will be 3x2 = 6 (3 2D embeddings)\n",
    "# num of neurons in this layer is a var up to us, we will use 100\n",
    "W1  = torch.randn((6,100))\n",
    "\n",
    "# biases\n",
    "b1 = torch.randn(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa02da3-4816-4536-8d82-2b02af495e1b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### what we want to do:\n",
    " ##### take input (i.e., embedding) and multiply it by weights and add bias\n",
    "emb @ W1 + b1\n",
    "\n",
    "### Problem, why we cant do this: The embeddings are stacked up in the dimensions of the tensor C\n",
    "\n",
    "### Solution: Concatenate inputs and transform [32,3,2] tensor into a [32,6] tensor\n",
    "\n",
    "There are many ways to do this in Torch, one way to concatenate the tensors in our input layer is using `torch.cat` (concatenates sequence of tensors in the given dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3315587b-89ed-4260-9bb0-8e594da2b6a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 6])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bad way to remove dimensions stacked up in tensor C and concatenate them.\n",
    "# bad because this is not generalizable/scalable, e.g., if we change the block size (i.e., context-length)\n",
    "\n",
    "torch.cat([emb[:,0,:],emb[:,1,:],emb[:,2,:]],1).shape # concatenates tensors along dimension 1 (second dimension)\n",
    "# essentially plucks out the 32 3x2 matrices and concatenates them so that we have 32 vectors with 6 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b796193f-356a-4155-a9c5-a2b68f287e4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.6435,  0.6752, -0.6435,  0.6752, -0.6435,  0.6752],\n",
       "        [-0.6435,  0.6752, -0.6435,  0.6752,  0.3085,  0.3569],\n",
       "        [-0.6435,  0.6752,  0.3085,  0.3569,  1.1798, -1.6131],\n",
       "        [ 0.3085,  0.3569,  1.1798, -1.6131,  1.1798, -1.6131],\n",
       "        [ 1.1798, -1.6131,  1.1798, -1.6131,  1.4563,  0.8970],\n",
       "        [-0.6435,  0.6752, -0.6435,  0.6752, -0.6435,  0.6752],\n",
       "        [-0.6435,  0.6752, -0.6435,  0.6752,  0.4964, -0.5908],\n",
       "        [-0.6435,  0.6752,  0.4964, -0.5908, -1.3122,  1.0173],\n",
       "        [ 0.4964, -0.5908, -1.3122,  1.0173,  2.0933, -1.2628],\n",
       "        [-1.3122,  1.0173,  2.0933, -1.2628, -0.2332, -1.2146],\n",
       "        [ 2.0933, -1.2628, -0.2332, -1.2146,  2.0933, -1.2628],\n",
       "        [-0.2332, -1.2146,  2.0933, -1.2628,  1.4563,  0.8970],\n",
       "        [-0.6435,  0.6752, -0.6435,  0.6752, -0.6435,  0.6752],\n",
       "        [-0.6435,  0.6752, -0.6435,  0.6752,  1.4563,  0.8970],\n",
       "        [-0.6435,  0.6752,  1.4563,  0.8970, -0.2332, -1.2146],\n",
       "        [ 1.4563,  0.8970, -0.2332, -1.2146,  1.4563,  0.8970],\n",
       "        [-0.6435,  0.6752, -0.6435,  0.6752, -0.6435,  0.6752],\n",
       "        [-0.6435,  0.6752, -0.6435,  0.6752,  2.0933, -1.2628],\n",
       "        [-0.6435,  0.6752,  2.0933, -1.2628, -1.1662, -1.1034],\n",
       "        [ 2.0933, -1.2628, -1.1662, -1.1034,  1.4563,  0.8970],\n",
       "        [-1.1662, -1.1034,  1.4563,  0.8970, -1.3446, -0.6647],\n",
       "        [ 1.4563,  0.8970, -1.3446, -0.6647,  0.3085,  0.3569],\n",
       "        [-1.3446, -0.6647,  0.3085,  0.3569, -1.3122,  1.0173],\n",
       "        [ 0.3085,  0.3569, -1.3122,  1.0173, -1.3122,  1.0173],\n",
       "        [-1.3122,  1.0173, -1.3122,  1.0173,  1.4563,  0.8970],\n",
       "        [-0.6435,  0.6752, -0.6435,  0.6752, -0.6435,  0.6752],\n",
       "        [-0.6435,  0.6752, -0.6435,  0.6752, -1.1662, -1.1034],\n",
       "        [-0.6435,  0.6752, -1.1662, -1.1034,  0.4964, -0.5908],\n",
       "        [-1.1662, -1.1034,  0.4964, -0.5908, -0.7251, -0.1117],\n",
       "        [ 0.4964, -0.5908, -0.7251, -0.1117,  1.4789,  0.6917],\n",
       "        [-0.7251, -0.1117,  1.4789,  0.6917,  2.0933, -1.2628],\n",
       "        [ 1.4789,  0.6917,  2.0933, -1.2628,  1.4563,  0.8970]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# better way to remove a Tensor dimension (pluck out the 32 3x2 matrices)\n",
    "tlist = torch.unbind(emb,1)\n",
    "# above is equivalent to list [emb[:,0,:],emb[:,1,:],emb[:,2,:]]\n",
    "torch.cat(tlist,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6cd098-f6f2-43a1-93d6-b18790150a6e",
   "metadata": {},
   "source": [
    "## An even better way to pop out dimensions from tensors and concatenate them \n",
    "*which we need to multiply the input embedding matrix by weights and add bias for our hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4275b5-aa7e-4b81-8d59-d61815675921",
   "metadata": {},
   "source": [
    "### Notes on how tensors are implemented in PyTorch\n",
    "\n",
    "n-dimensional tensors are simply represented as a 1-Dimensional array/vector of values (e.g., ints)\n",
    "\n",
    "`.view()` allows us to interpret any 1-D array of values as an n-dimensional tensor. This works as long as the sum of the number of dimensions is equal to the sum of the number elements\n",
    "- When using `.view` no memory is being changed, copied, moved or createdâ€“<mark>so this operation is extremely efficient</mark>. The storage is identical to the 1-D representation. Storage offset, strides and shapes are manipulated so that the 1-D sequence of bytes is seen as a n-dimensional arrays\n",
    "\n",
    "See: [PyTorch Internals](http://blog.ezyang.com/2019/05/pytorch-internals/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b5f0704-1acf-4739-ab31-8a16e099fb75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# below effectively does the same as before (pops out dimensions and stacks them up in a single row)\n",
    "emb.view(32,6) == torch.cat(torch.unbind(emb,1),1) # verifies via element-wise equals operation that two methods output same result\n",
    "\n",
    "# CONCATENATION IS MUCH LESS EFFICIENT BECAUSE IT CREATES A WHOLE NEW TENSOR WITH NEW STORAGE\n",
    "# - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da23c2a-4278-46dc-8df0-d0ee30af4529",
   "metadata": {},
   "source": [
    "#### # CONCATENATION IS MUCH LESS EFFICIENT\n",
    "- It creates a new tensor (new memory is being created), because there's no way to concanetate tensors just by manupulating view attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b64d86b7-c866-41b5-b74e-b0e66f2f0a9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.5245,  0.9325,  0.1768,  ..., -0.9681,  0.6484,  0.7924],\n",
       "        [-0.9584,  0.9922,  0.3813,  ..., -0.7994, -0.1910,  0.9940],\n",
       "        [-0.9999,  0.9957,  0.5736,  ..., -0.1149,  0.6913,  1.0000],\n",
       "        ...,\n",
       "        [-0.7078,  0.9997,  0.9851,  ...,  0.9277, -0.9998,  0.9919],\n",
       "        [-0.9938,  0.6503,  0.5311,  ...,  0.1070,  0.2070,  1.0000],\n",
       "        [-0.9494,  0.9758,  0.9999,  ...,  0.4790, -0.8620,  0.9999]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And now it works!\n",
    "h = torch.tanh(emb.view(emb.shape[0],6) @ W1 + b1)\n",
    "# hidden layer creates 100 activations for every one of our (32 or whatever the value of first dimension is) activations\n",
    "# also adds tanh nonlinearity\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b29fe5d-0eeb-4e33-99f5-6fc458853f7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we can also us [-1] dimension for PyTorch to infer the correct dimension to use\n",
    "# h = torch.tanh(emb.view(-1,6) @ W1 + b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce1c17d-cf2e-4e02-b1cf-dc2889507f85",
   "metadata": {},
   "source": [
    "### How the 1-D array of biases are added to the tensor in hidden layer\n",
    "*Review of how broadcasting works\n",
    "\n",
    "Basically if (after our embedding matrix gets multiplied by the weight matrix) its a [32,100] matrix, then the vector [100] gets broadcasted as a [1,100] tensor. So it will get copied for each of the 32 rows and do an element wise addition. I.e., the same bias vector will be added to all the rows of the embedding matrix.\n",
    "\n",
    "So the addition will look like:\n",
    " [32,100]\n",
    "+[1,100]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3172f320-de4a-499d-8f55-03fb6415ae51",
   "metadata": {},
   "source": [
    "## Next: Create output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c4d96a7-3ffc-4b5b-8a15-20e86276502c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  create W2 and B2 for output layer\n",
    "W2  = torch.randn((100,27)) # take 100 neurons from hidden layer as input, and output 27, # of possible chars\n",
    "\n",
    "# biases\n",
    "b1 = torch.randn(100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
