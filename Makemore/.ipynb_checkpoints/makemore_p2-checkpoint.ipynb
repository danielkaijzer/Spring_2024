{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "fda81484-ffba-4c53-8ea9-87eef71425b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# get words from data set\n",
    "words = open('names.txt','r').read().splitlines() # convert txt file to list of strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "cf458d44-e728-4475-bee5-8d500343f75d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "a5b55d3f-309c-40d7-9ddc-50a0d17d98d8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num of examples:  228146\n"
     ]
    }
   ],
   "source": [
    "# create the dataset of bigrams\n",
    "xs, ys = [], []\n",
    "\n",
    "for w in words: # iterate over words\n",
    "    chs = ['.'] + list(w) + ['.']\n",
    "    \n",
    "    for ch1, ch2 in zip(chs, chs[1:]):\n",
    "        ix1 = stoi[ch1]\n",
    "        ix2 = stoi[ch2]\n",
    "        xs.append(ix1)\n",
    "        ys.append(ix2)\n",
    "xs = torch.tensor(xs)\n",
    "ys = torch.tensor(ys)\n",
    "num = xs.nelement()\n",
    "print('num of examples: ', num)\n",
    "\n",
    "# init network\n",
    "g = torch.Generator().manual_seed(2147483647) # use consistent random seed\n",
    "W = torch.randn((27,27), generator=g, requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "f9e6f71a-cf71-41af-9e52-018907f99d4f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4706625938415527\n",
      "2.4704854488372803\n",
      "2.4703118801116943\n",
      "2.4701411724090576\n",
      "2.4699742794036865\n",
      "2.4698104858398438\n",
      "2.4696500301361084\n",
      "2.4694924354553223\n",
      "2.4693377017974854\n",
      "2.4691860675811768\n",
      "2.4690372943878174\n",
      "2.468890905380249\n",
      "2.468747615814209\n",
      "2.468606948852539\n",
      "2.468468427658081\n",
      "2.468332529067993\n",
      "2.4681990146636963\n",
      "2.4680681228637695\n",
      "2.4679393768310547\n",
      "2.4678127765655518\n",
      "2.46768856048584\n",
      "2.4675662517547607\n",
      "2.4674463272094727\n",
      "2.4673283100128174\n",
      "2.467211961746216\n",
      "2.467097759246826\n",
      "2.4669854640960693\n",
      "2.4668750762939453\n",
      "2.466766595840454\n",
      "2.4666597843170166\n",
      "2.466554641723633\n",
      "2.4664509296417236\n",
      "2.4663491249084473\n",
      "2.4662492275238037\n",
      "2.4661505222320557\n",
      "2.466053009033203\n",
      "2.4659576416015625\n",
      "2.4658637046813965\n",
      "2.4657704830169678\n",
      "2.46567964553833\n",
      "2.4655895233154297\n",
      "2.465500593185425\n",
      "2.4654135704040527\n",
      "2.465327262878418\n",
      "2.465242385864258\n",
      "2.4651591777801514\n",
      "2.4650766849517822\n",
      "2.4649956226348877\n",
      "2.4649155139923096\n",
      "2.464836597442627\n",
      "2.46475887298584\n",
      "2.46468186378479\n",
      "2.464606285095215\n",
      "2.464531660079956\n",
      "2.4644582271575928\n",
      "2.464385747909546\n",
      "2.4643142223358154\n",
      "2.464243173599243\n",
      "2.4641737937927246\n",
      "2.4641048908233643\n",
      "2.4640369415283203\n",
      "2.4639699459075928\n",
      "2.4639039039611816\n",
      "2.4638383388519287\n",
      "2.463773727416992\n",
      "2.463710308074951\n",
      "2.4636473655700684\n",
      "2.463585376739502\n",
      "2.4635238647460938\n",
      "2.463463544845581\n",
      "2.4634037017822266\n",
      "2.4633448123931885\n",
      "2.4632863998413086\n",
      "2.463228702545166\n",
      "2.46317195892334\n",
      "2.4631154537200928\n",
      "2.463059902191162\n",
      "2.4630050659179688\n",
      "2.4629507064819336\n",
      "2.4628970623016357\n",
      "2.462843894958496\n",
      "2.462791681289673\n",
      "2.462739944458008\n",
      "2.462688684463501\n",
      "2.4626379013061523\n",
      "2.462587833404541\n",
      "2.462538003921509\n",
      "2.462489366531372\n",
      "2.4624407291412354\n",
      "2.462392807006836\n",
      "2.462345600128174\n",
      "2.4622981548309326\n",
      "2.462252140045166\n",
      "2.4622061252593994\n",
      "2.462161064147949\n",
      "2.462116003036499\n",
      "2.462071657180786\n",
      "2.4620275497436523\n",
      "2.461984157562256\n",
      "2.4619410037994385\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "for k in range(100):\n",
    "    \n",
    "    # forward pass\n",
    "    xenc = F.one_hot(xs, num_classes=27).float() # input to the network: one-hot encoding\n",
    "    logits = xenc @ W # predict log-counts\n",
    "    counts = logits.exp() # counts, equivalent to N\n",
    "    probs = counts / counts.sum(1, keepdims=True) # probabilities for next character\n",
    "    loss = -probs[torch.arange(num),ys].log().mean()\n",
    "    print(loss.item())\n",
    "    \n",
    "    # backward pass\n",
    "    W.grad = None # set gradient to zero\n",
    "    loss.backward()\n",
    "    \n",
    "    # update weights using gradient data\n",
    "    W.data += -50 * W.grad\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "2099b8c6-ee84-45c5-a04b-ee51b6a0a97a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Note: logits (xenc @ W) effectively indexes into the row of the W matrix(at the one-hot location)\n",
    "# and becomes the appropriate row of Wlog  (of probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "a66d73ca-bd51-4e1e-93c4-17546d395c15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Note 2: to achieve model smoothing in NN:\n",
    "# the closer the weights in W are to each other, \n",
    "# or especially if all values of W are equal to 0,\n",
    "# the more uniform the probabilities \n",
    "# will be for all 27 characters\n",
    "\n",
    "# this is called regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "55ae6d20-7731-4817-b17f-3daaac9853ff",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cexze.\n",
      "momasurailezityha.\n",
      "konimittain.\n",
      "llayn.\n",
      "ka.\n"
     ]
    }
   ],
   "source": [
    "# Sampling from the neural network model\n",
    "\n",
    "g = torch.Generator().manual_seed(2147483647)\n",
    "\n",
    "for i in range(5):\n",
    "    \n",
    "    out = []\n",
    "    ix = 0\n",
    "    while True:\n",
    "        xenc = F.one_hot(torch.tensor([ix]), num_classes=27).float()\n",
    "        logits = xenc @ W # predict log-counts\n",
    "        counts = logits.exp() # counts, equivalent to N\n",
    "        p = counts / counts.sum(1, keepdims=True) # prob of next char\n",
    "        \n",
    "        ix = torch.multinomial(p, num_samples=1, replacement=True, generator=g).item()\n",
    "        out.append(itos[ix])\n",
    "        if ix == 0:\n",
    "            break\n",
    "    print(''.join(out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
