{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "772f530d-001b-4052-b935-6c81d3abebfa",
   "metadata": {},
   "source": [
    "## Problem: \n",
    "    we want to solve: How to take larger contexts in a manageable way. \n",
    "    We cannot just use tables like we did with bigrams (context of one char to predict next char) because they grow exponentially. \n",
    "## Solution:\n",
    "    We will use approach used in Bengio et al. paper. We will embed chars into a n dimensional vector space (a space smaller than number of chars which is 27). \n",
    "    \n",
    "    In this space related chars will be clustered together within the vector space, allowing us to generalize relationships so we can predict next chars in novel starting points.\n",
    "    \n",
    "    At first the embeddings will be random, i.e., the vectors representing chars will be placed randomly within the vector space. Then, using backpropagation in our multilayer neural network, we will change the locations of the vectors within the vector space so that chars that are often combined (in a context of n characters) are placed closely together.\n",
    "    \n",
    "    \"We can, through the embedding space, transfer knowledge and generalize to novel scenarios.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d45aded-5db9-44d9-9f49-bec37b7e9a0b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### What our neural network architecture looks like:\n",
    "\n",
    "It will essentially consist of 3 main layers:\n",
    "1. An input layer\n",
    "    - where the size depends on: \n",
    "        1. n - the context length (how many chars we will use to predict next char)\n",
    "        2. m - the number of dimensions of the vector space we will use to embed chars (i.e., represent unique chars as a vector of m numbers).\n",
    "    - number of neurons in our input layer is n * m\n",
    "    - this first layer has linear neurons (i.e., neurons with no non-linearity) and their weight matrix is the LUT C (the embedding matrix). We are essentially encoding ints into one-hot, embedding them into the LUT, and then feeding the embedding LUT C into the next layer of our NN (which is 2. the hidden layer)\n",
    "2. A \"hidden layer\" that has a tanh non-linearity. This layer is fully connected to our input layer\n",
    "    - i.e., fully connected to the numbers representing the n chars used as inputs. So the n*m inputs will be fed into the tanh layer\n",
    "3. An output layer \n",
    "    - all neurons of output layer are fully connected to all neurons of tanh (non-linearity) layer\n",
    "    - will output a char chosen from the 27 possible number of chars. The chosen char will be the char with the highest probability based off of the previous n chars. And this probability will be tuned over several epochs to minimize a loss value (which measures the quality of our NN).\n",
    "        - Note: the loss will use negative log likelihood\n",
    "    - Most computation will be in this output layer. So this layer is the most performance expensive layer of our NN.\n",
    "    - This layer will have 27 logits, so this layer will have softmax layer on top of it. Everyone of these logits gets exponentiated and they are normalized so all logits sum to 1. This will then give us a useful probability distribution for the next char in the sequence.\n",
    "        - During training we will have labels (the identity of the correct next char in the sequence). That char (represented as an int index) will be used to pluck out the probability of that char. The NN will be trained, i.e., the parameters of our NN will be tuned, to maximize the probability of that char (given the n previous chars in the sequence). \n",
    "\n",
    "Note: These parameters of our NN are the weights and biases of the output layer and our hidden layer, as well as the embedding LUT (C) within our input layer. All these parameters are optimized using backprop.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f864b016-f0e2-4b39-a5dd-5b67b370e962",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d9631da-2da8-4309-b72d-bd8bc57a8e84",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['emma', 'olivia', 'ava', 'isabella', 'sophia', 'charlotte', 'mia', 'amelia']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = open('names.txt', 'r').read().splitlines()\n",
    "words[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2ccdb0b-9e84-4da1-955b-1ef8c3def003",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32033"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(words) # print number of names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8edd6f3b-d607-4aa4-950c-59bcb3940889",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{1: 'a', 2: 'b', 3: 'c', 4: 'd', 5: 'e', 6: 'f', 7: 'g', 8: 'h', 9: 'i', 10: 'j', 11: 'k', 12: 'l', 13: 'm', 14: 'n', 15: 'o', 16: 'p', 17: 'q', 18: 'r', 19: 's', 20: 't', 21: 'u', 22: 'v', 23: 'w', 24: 'x', 25: 'y', 26: 'z', 0: '.'}\n"
     ]
    }
   ],
   "source": [
    "# build the vocab of chars and mappings to/from ints\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {s:i+1 for i,s in enumerate(chars)} # chars from 1-26 are alpha\n",
    "stoi['.'] = 0 # special char to delineate begin/end of word\n",
    "itos = {i:s for s,i in stoi.items()}\n",
    "print(itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b18c499-7285-41f7-9567-2901254ec446",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emma\n",
      "... ----> e\n",
      "..e ----> m\n",
      ".em ----> m\n",
      "emm ----> a\n",
      "mma ----> .\n",
      "olivia\n",
      "... ----> o\n",
      "..o ----> l\n",
      ".ol ----> i\n",
      "oli ----> v\n",
      "liv ----> i\n",
      "ivi ----> a\n",
      "via ----> .\n",
      "ava\n",
      "... ----> a\n",
      "..a ----> v\n",
      ".av ----> a\n",
      "ava ----> .\n",
      "isabella\n",
      "... ----> i\n",
      "..i ----> s\n",
      ".is ----> a\n",
      "isa ----> b\n",
      "sab ----> e\n",
      "abe ----> l\n",
      "bel ----> l\n",
      "ell ----> a\n",
      "lla ----> .\n",
      "sophia\n",
      "... ----> s\n",
      "..s ----> o\n",
      ".so ----> p\n",
      "sop ----> h\n",
      "oph ----> i\n",
      "phi ----> a\n",
      "hia ----> .\n"
     ]
    }
   ],
   "source": [
    "# build the dataset\n",
    "\n",
    "block_size = 3 # context length; num of previous tokens (chars) used to predict next token (char)\n",
    "X, Y = [],[] # X contains inputs to NN, Y contains the labels for each example inside X\n",
    "\n",
    "for w in words[:5]: # loop through training set (or subset of training set)\n",
    "    \n",
    "    print(w) # print entire word (name)\n",
    "    context = [0] * block_size # padded tokens, remember that 0 maps to '.' (special char)\n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch] # enumerate chars; convert ch to int val\n",
    "        X.append(context) # build out array X which stores running context\n",
    "        Y.append(ix) # build out array Y for current character\n",
    "        print(''.join(itos[i] for i in context), '---->', itos[ix])\n",
    "        context = context[1:] + [ix] # crop and append (rolling window of context), decrement padding as chars are appended\n",
    "        # remember location 1 above refers to second char in string\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "456b7f2c-6fbf-4a27-b9ac-2f0f0587f4b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 3]), torch.int64, torch.Size([32]), torch.int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, X.dtype, Y.shape, Y.dtype\n",
    "# from the 5 words (inputs) we have created a data set of 32 examples\n",
    "# and each input to the NN is 3 integers \n",
    "# label Y is also an integer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "598135ab-d017-4604-a912-a3787d1784d7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  0,  0],\n",
       "        [ 0,  0,  5],\n",
       "        [ 0,  5, 13],\n",
       "        [ 5, 13, 13],\n",
       "        [13, 13,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0, 15],\n",
       "        [ 0, 15, 12],\n",
       "        [15, 12,  9],\n",
       "        [12,  9, 22],\n",
       "        [ 9, 22,  9],\n",
       "        [22,  9,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  1],\n",
       "        [ 0,  1, 22],\n",
       "        [ 1, 22,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0,  9],\n",
       "        [ 0,  9, 19],\n",
       "        [ 9, 19,  1],\n",
       "        [19,  1,  2],\n",
       "        [ 1,  2,  5],\n",
       "        [ 2,  5, 12],\n",
       "        [ 5, 12, 12],\n",
       "        [12, 12,  1],\n",
       "        [ 0,  0,  0],\n",
       "        [ 0,  0, 19],\n",
       "        [ 0, 19, 15],\n",
       "        [19, 15, 16],\n",
       "        [15, 16,  8],\n",
       "        [16,  8,  9],\n",
       "        [ 8,  9,  1]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X # X is a n x m matrix\n",
    "# n = number of input chars (examples) and m is context length (num of chars used to predict next char)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c810cd-d742-4165-91e8-1d3f9ccc34d2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Task: Build NN that takes Xs and predicts Ys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00f9712-8f68-4fd6-827c-e9c228ff6d89",
   "metadata": {},
   "source": [
    "## How embedding chars into the Look Up Table works\n",
    "\n",
    "We will use a char (an int between 0-26) to index into a $27*m$ matrix that represents a LUT, where m is the number of dimensions of a vector space we want to embed chars into.\n",
    "\n",
    "Every index (representing a char) is plucking out a row of the embedding matrix, so that each index is converted into the m-dimensional vector that corresponds to the embedding vector for that char.\n",
    "\n",
    "So basically embedding means we will represent an int (which itself represents a char) as a vector of m numbers (assuming float values). So if we have a context length of 3 chars and m = 30. Our input layer will have 90 numbers that represent 3 chars.\n",
    "\n",
    "The same LUT matrix will be shared by all chars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a1ba86-7306-43ca-98b4-859c05786863",
   "metadata": {},
   "source": [
    "### Build lookup table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cbc79b5b-8ef4-4404-b717-321859294b1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# just like Bengio et al. paper, we will embed 27 possible chars\n",
    "# in smaller dimensional space\n",
    "\n",
    "# each of the 27 chars will have a 2-dimensional embedding\n",
    "C = torch.randn([27,2]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5bb0bf7-0258-436b-af35-99eeca04ec51",
   "metadata": {},
   "source": [
    "#### We will embedd all ints inside input X into LUT C \n",
    "\n",
    "There are two ways to embed a single int (e.g., 5) into LUT C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9eb0f3-ad28-4dcb-8d88-ae5c686a71ee",
   "metadata": {},
   "source": [
    "#### 1) take C and index into row 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71bbb42e-ba88-4303-ae43-7de88b57157c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7904, 0.4452])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0909044e-f23f-40f8-b106-e5fa9413c6cb",
   "metadata": {},
   "source": [
    "##### Method 2: use one-hot encoding \n",
    "- Even though it may not immediately seem like it, this method is actually identical to first method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a16a793f-c4a3-458d-8433-c428569acffa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7904, 0.4452])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a 27-D vector (i.e., array of 27 nums) with all 0s except for the 5th element in vector which is 1\n",
    "# i.e., 5th dimension is 1\n",
    "# we then cast to float() so we can multiply by matrix C\n",
    "\n",
    "# the one-hot encoding will essentially pluck out 5th row of C\n",
    "# because 0s mask the other rows\n",
    "# giving us the same result as the first method for embeding an int above\n",
    "F.one_hot(torch.tensor(5), num_classes=27).float() @ C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1375406a-1574-4355-b0fd-d07d4902999f",
   "metadata": {},
   "source": [
    "#### For efficiency we will use first method for embedding (just indexing into row of LUT C)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6469c9-9d19-41fe-9a79-6dda32906ab4",
   "metadata": {},
   "source": [
    "Even easier, We can embed using tensors to simultaneously embed all 32x3 input ints (in array X) into the LUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "39ae6d71-98fc-4cc5-93e4-934f2a42ad12",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# C[X] # prints the embedding of inputs tensor X\n",
    "# C[X].shape # [32,3,2] # 32 input nums, context length 3, embedded into 2 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f3cdc82-253f-42ad-9854-eb189732bb69",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[13,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "926173ba-8d40-4dbc-9071-d079e2eb8479",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2227,  0.8875])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[X][13,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01ac5411-d57e-4ad4-a722-9d0738aac3d0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.2227,  0.8875])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebe6f9b4-2786-429f-b645-03618f587057",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X]\n",
    "emb.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fcdcba7-fb03-40b3-8990-71291992c84a",
   "metadata": {},
   "source": [
    "## Construct hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c3ee453b-1731-4855-b847-23c131cea8fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# weights, the num of inputs to this layer will be 3x2 = 6 (3 2D embeddings)\n",
    "# num of neurons in this layer is a var up to us, we will use 100\n",
    "W1  = torch.randn((6,100))\n",
    "\n",
    "# biases\n",
    "b1 = torch.randn(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa02da3-4816-4536-8d82-2b02af495e1b",
   "metadata": {
    "tags": []
   },
   "source": [
    "### what we want to do:\n",
    " ##### take input (i.e., embedding) and multiply it by weights and add bias\n",
    "emb @ W1 + b1\n",
    "\n",
    "### Problem, why we cant do this: The embeddings are stacked up in the dimensions of the tensor C\n",
    "\n",
    "### Solution: Concatenate inputs and transform [32,3,2] tensor into a [32,6] tensor\n",
    "\n",
    "There are many ways to do this in Torch, one way to concatenate the tensors in our input layer is using `torch.cat` (concatenates sequence of tensors in the given dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3315587b-89ed-4260-9bb0-8e594da2b6a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 6])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# bad way to remove dimensions stacked up in tensor C and concatenate them.\n",
    "# bad because this is not generalizable/scalable, e.g., if we change the block size (i.e., context-length)\n",
    "\n",
    "torch.cat([emb[:,0,:],emb[:,1,:],emb[:,2,:]],1).shape # concatenates tensors along dimension 1 (second dimension)\n",
    "# essentially plucks out the 32 3x2 matrices and concatenates them so that we have 32 vectors with 6 elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b796193f-356a-4155-a9c5-a2b68f287e4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.2375, -0.2146, -1.2375, -0.2146, -1.2375, -0.2146],\n",
       "        [-1.2375, -0.2146, -1.2375, -0.2146,  0.7904,  0.4452],\n",
       "        [-1.2375, -0.2146,  0.7904,  0.4452, -0.3033, -0.2426],\n",
       "        [ 0.7904,  0.4452, -0.3033, -0.2426, -0.3033, -0.2426],\n",
       "        [-0.3033, -0.2426, -0.3033, -0.2426, -0.2227,  0.8875],\n",
       "        [-1.2375, -0.2146, -1.2375, -0.2146, -1.2375, -0.2146],\n",
       "        [-1.2375, -0.2146, -1.2375, -0.2146, -0.3327, -0.3162],\n",
       "        [-1.2375, -0.2146, -0.3327, -0.3162, -0.4409, -1.5052],\n",
       "        [-0.3327, -0.3162, -0.4409, -1.5052, -1.2988, -0.6965],\n",
       "        [-0.4409, -1.5052, -1.2988, -0.6965,  0.6664,  0.9810],\n",
       "        [-1.2988, -0.6965,  0.6664,  0.9810, -1.2988, -0.6965],\n",
       "        [ 0.6664,  0.9810, -1.2988, -0.6965, -0.2227,  0.8875],\n",
       "        [-1.2375, -0.2146, -1.2375, -0.2146, -1.2375, -0.2146],\n",
       "        [-1.2375, -0.2146, -1.2375, -0.2146, -0.2227,  0.8875],\n",
       "        [-1.2375, -0.2146, -0.2227,  0.8875,  0.6664,  0.9810],\n",
       "        [-0.2227,  0.8875,  0.6664,  0.9810, -0.2227,  0.8875],\n",
       "        [-1.2375, -0.2146, -1.2375, -0.2146, -1.2375, -0.2146],\n",
       "        [-1.2375, -0.2146, -1.2375, -0.2146, -1.2988, -0.6965],\n",
       "        [-1.2375, -0.2146, -1.2988, -0.6965, -1.9342, -1.2390],\n",
       "        [-1.2988, -0.6965, -1.9342, -1.2390, -0.2227,  0.8875],\n",
       "        [-1.9342, -1.2390, -0.2227,  0.8875, -0.0072, -1.1483],\n",
       "        [-0.2227,  0.8875, -0.0072, -1.1483,  0.7904,  0.4452],\n",
       "        [-0.0072, -1.1483,  0.7904,  0.4452, -0.4409, -1.5052],\n",
       "        [ 0.7904,  0.4452, -0.4409, -1.5052, -0.4409, -1.5052],\n",
       "        [-0.4409, -1.5052, -0.4409, -1.5052, -0.2227,  0.8875],\n",
       "        [-1.2375, -0.2146, -1.2375, -0.2146, -1.2375, -0.2146],\n",
       "        [-1.2375, -0.2146, -1.2375, -0.2146, -1.9342, -1.2390],\n",
       "        [-1.2375, -0.2146, -1.9342, -1.2390, -0.3327, -0.3162],\n",
       "        [-1.9342, -1.2390, -0.3327, -0.3162,  0.4980,  0.4330],\n",
       "        [-0.3327, -0.3162,  0.4980,  0.4330,  0.5367, -0.4627],\n",
       "        [ 0.4980,  0.4330,  0.5367, -0.4627, -1.2988, -0.6965],\n",
       "        [ 0.5367, -0.4627, -1.2988, -0.6965, -0.2227,  0.8875]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# better way to remove a Tensor dimension (pluck out the 32 3x2 matrices)\n",
    "tlist = torch.unbind(emb,1)\n",
    "# above is equivalent to list [emb[:,0,:],emb[:,1,:],emb[:,2,:]]\n",
    "torch.cat(tlist,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6cd098-f6f2-43a1-93d6-b18790150a6e",
   "metadata": {},
   "source": [
    "## An even better way to pop out dimensions from tensors and concatenate them \n",
    "*which we need to multiply the input embedding matrix by weights and add bias for our hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4275b5-aa7e-4b81-8d59-d61815675921",
   "metadata": {},
   "source": [
    "### Notes on how tensors are implemented in PyTorch\n",
    "\n",
    "n-dimensional tensors are simply represented as a 1-Dimensional array/vector of values (e.g., ints)\n",
    "\n",
    "`.view()` allows us to interpret any 1-D array of values as an n-dimensional tensor. This works as long as the sum of the number of dimensions is equal to the sum of the number elements\n",
    "- When using `.view` no memory is being changed, copied, moved or createdâ€“<mark>so this operation is extremely efficient</mark>. The storage is identical to the 1-D representation. Storage offset, strides and shapes are manipulated so that the 1-D sequence of bytes is seen as a n-dimensional arrays\n",
    "\n",
    "See: [PyTorch Internals](http://blog.ezyang.com/2019/05/pytorch-internals/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5b5f0704-1acf-4739-ab31-8a16e099fb75",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# below effectively does the same as before (pops out dimensions and stacks them up in a single row)\n",
    "emb.view(32,6) == torch.cat(torch.unbind(emb,1),1) # verifies via element-wise equals operation that two methods output same result\n",
    "\n",
    "# CONCATENATION IS MUCH LESS EFFICIENT BECAUSE IT CREATES A WHOLE NEW TENSOR WITH NEW STORAGE\n",
    "# - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da23c2a-4278-46dc-8df0-d0ee30af4529",
   "metadata": {},
   "source": [
    "#### # CONCATENATION IS MUCH LESS EFFICIENT\n",
    "- It creates a new tensor (new memory is being created), because there's no way to concanetate tensors just by manupulating view attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b64d86b7-c866-41b5-b74e-b0e66f2f0a9d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1043, -0.1835,  0.7867,  ...,  0.9024,  0.9541, -0.9573],\n",
       "        [-0.2875,  0.8665,  0.6479,  ...,  0.9938, -0.7915, -0.9739],\n",
       "        [ 0.9569,  0.5777,  0.2254,  ..., -0.9936,  0.2952,  0.1256],\n",
       "        ...,\n",
       "        [ 0.5046,  0.9212, -0.7307,  ..., -0.7304,  0.1266,  0.1006],\n",
       "        [-0.5776, -0.3041, -0.7648,  ..., -0.2129,  0.9986,  0.9992],\n",
       "        [-0.7732, -0.0461, -0.9012,  ...,  0.9806,  0.9289,  0.8661]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And now it works!\n",
    "h = torch.tanh(emb.view(emb.shape[0],6) @ W1 + b1)\n",
    "# hidden layer creates 100 activations for every one of our (32 or whatever the value of first dimension is) activations\n",
    "# also adds tanh nonlinearity\n",
    "h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0b29fe5d-0eeb-4e33-99f5-6fc458853f7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# we can also us [-1] dimension for PyTorch to infer the correct dimension to use\n",
    "# h = torch.tanh(emb.view(-1,6) @ W1 + b1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce1c17d-cf2e-4e02-b1cf-dc2889507f85",
   "metadata": {},
   "source": [
    "### How the 1-D array of biases are added to the tensor in hidden layer\n",
    "*Review of how broadcasting works\n",
    "\n",
    "Basically if (after our embedding matrix gets multiplied by the weight matrix) its a [32,100] matrix, then the vector [100] gets broadcasted as a [1,100] tensor. So it will get copied for each of the 32 rows and do an element wise addition. I.e., the same bias vector will be added to all the rows of the embedding matrix.\n",
    "\n",
    "So the addition will look like:\n",
    " [32,100]\n",
    "+[1,100]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3172f320-de4a-499d-8f55-03fb6415ae51",
   "metadata": {},
   "source": [
    "## Next: Create output layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c4d96a7-3ffc-4b5b-8a15-20e86276502c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  create W2 and B2 for output layer\n",
    "W2  = torch.randn((100,27)) # take 100 neurons from hidden layer as input, and output 27, # of possible chars\n",
    "\n",
    "# biases\n",
    "b2 = torch.randn(27) # 27 because output is 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "860842f9-b35c-440b-9203-cdfb7b49593c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "logits = h @ W2 + b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b91f6d27-b503-491e-b8a0-08706f190775",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 27])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "610fa7e1-0b24-49e7-bb08-7cf40fbaba43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "counts = logits.exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "50f97a27-d934-4ad6-a76c-4e77b5992129",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prob = counts / counts.sum(1,keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9ef078eb-7cfd-493b-9454-fe994d9b239a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0000)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ed17a722-a721-4303-a7c0-5e1bae4073b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(11.0109)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this gives current probabilities to labels (correct char in sequence) given current weights of NN\n",
    "#prob[torch.arange(32),Y]\n",
    "\n",
    "# calculate negative log likelihood loss value\n",
    "loss = -prob[torch.arange(32),Y].log().mean()\n",
    "loss # we want to minimize this loss to get NN to predict correct next  chars in sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c91620-f5d7-47b6-ade4-544128e000af",
   "metadata": {},
   "source": [
    "## Cleaner version of NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "681f7b5b-217e-4693-a5a5-e91cb1bb8592",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# params\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C = torch.randn((27,2), generator=g)\n",
    "\n",
    "W1 = torch.randn((6,100), generator=g) # 6 inputs 100 outputs\n",
    "b1 = torch.randn(100, generator=g) # will be broadcasted when added to W1\n",
    "\n",
    "W2 = torch.randn((100,27), generator=g) # 100 outputs from hidden layer, 27 outputs (27 chars possible)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C,W1,b1,W2,b2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "146fea18-d003-4bb4-a29e-cc49cacd59ef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3481"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.nelement() for p in parameters) # num of total params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8d32e224-d923-4aaf-8e96-beeb98385ddf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(17.7697)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb = C[X] # build LUT embedding matrix, (32,3,2)\n",
    "h = torch.tanh(emb.view(-1,6) @ W1 + b1) # (32,100) hidden layer\n",
    "logits = h @ W2 + b2 # (32,27)\n",
    "counts = logits.exp()\n",
    "prob = counts/counts.sum(1,keepdims=True)\n",
    "loss = -prob[torch.arange(32),Y].log().mean()\n",
    "loss # measures how well NN predicts correct output (next char in sequence) given its current parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0cd6a9-985e-40c5-ad04-bdb50654f2e2",
   "metadata": {},
   "source": [
    "### easier, more efficient way to calculate loss using logits and Y (output labels)\n",
    "`cross_entropy()` is more efficient because:\n",
    "1) it avoids copying and creating new tensors in memory which above method requires. Instead PyTorch, when calling cross_entropy, will cluster up these operations above and use \"fused kernels\" that efficiently evaluate these expressions (clustered mathematical operations)\n",
    "2) The backward pass can be made much more efficient, due to the clustered mathematical expression its simpler and easier to implement the backward pass\n",
    "3) Under the hood, `cross_entropy` is more numerically well-behaved\n",
    "- PyTorch solves floating point bad behavior from when logits have extreme values (e.g., 2^100 will be out of range for float values so it will ruin the probability distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a31f9bad-1e52-48d0-94eb-e353190002bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# before beginning training loop make sure that for all parameters\n",
    "# p.requires_grad is set to True\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b620ecef-c943-4a11-972e-bb31ee9c4686",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.76971435546875\n",
      "13.656402587890625\n",
      "11.298770904541016\n",
      "9.452458381652832\n",
      "7.984263896942139\n",
      "6.891323089599609\n",
      "6.100015640258789\n",
      "5.452036380767822\n",
      "4.8981523513793945\n",
      "4.414664268493652\n"
     ]
    }
   ],
   "source": [
    "# TRAINING LOOP\n",
    "for _ in range(10):\n",
    "    # forward pass\n",
    "    emb = C[X] # build LUT embedding matrix, (num of examples,3,2)\n",
    "    h = torch.tanh(emb.view(-1,6) @ W1 + b1) # (32,100) hidden layer\n",
    "    logits = h @ W2 + b2 # (32,27)\n",
    "    loss = F.cross_entropy(logits,Y) # replaces counts, prob and loss lines above\n",
    "    print(loss.item())\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None # initialize grad vals to 0\n",
    "    loss.backward() # populates gradients\n",
    "\n",
    "    # update parameters\n",
    "    for p in parameters:\n",
    "        p.data += -0.1 * p.grad \n",
    "        \n",
    "#print(loss.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca835692-4bcd-49bf-84c5-ef227235362e",
   "metadata": {},
   "source": [
    "### ABOVE: we are \"overfitting to one batch of data\". \n",
    "I.e., we are only tuning the weights of our NN to predict the next char given prev chars for 32 char examples from our data set.\n",
    "\n",
    "### ALSO: We cannot achieve a loss of 0 because:\n",
    "There are cases where a unique input doesn't have a unique output. E.g., if there are no previous chars, there's an equal likelihood of it being any of the starting chars in our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e46e81ca-da99-48df-9d74-7de15130232d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# UPDATING TO TRAIN ENTIRE DATASET\n",
    "\n",
    "# build the dataset\n",
    "\n",
    "block_size = 3 # context length; num of previous tokens (chars) used to predict next token (char)\n",
    "X, Y = [],[] # X contains inputs to NN, Y contains the labels for each example inside X\n",
    "\n",
    "for w in words: # loop through training set (or subset of training set)\n",
    "    \n",
    "    #print(w) # print entire word (name)\n",
    "    context = [0] * block_size # padded tokens, remember that 0 maps to '.' (special char)\n",
    "    for ch in w + '.':\n",
    "        ix = stoi[ch] # enumerate chars; convert ch to int val\n",
    "        X.append(context) # build out array X which stores running context\n",
    "        Y.append(ix) # build out array Y for current character\n",
    "        #print(''.join(itos[i] for i in context), '---->', itos[ix])\n",
    "        context = context[1:] + [ix] # crop and append (rolling window of context), decrement padding as chars are appended\n",
    "        # remember location 1 above refers to second char in string\n",
    "\n",
    "X = torch.tensor(X)\n",
    "Y = torch.tensor(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a243ba27-40d4-42a1-9536-ef0e0bb73d42",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([228146, 3]), torch.Size([228146]))"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape # now we have 228146 inputs and outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27970ef4-141b-465d-b743-06788a4f0ca1",
   "metadata": {},
   "source": [
    "### Addressing how long it takes to train on whole dataset.\n",
    "\n",
    "### SOLUTION: Train on (random) minibatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "faa991ab-336b-4fd2-96b9-dd855e0229fd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([186413, 116734,  16657,  27844,  88370, 157847, 213617, 181393, 195626,\n",
       "         91773,  54740,   6312,  61600, 109398,  53959, 141761,  65192, 125395,\n",
       "         73008,  30814,  20444, 149805, 144043,  77028, 162754, 146268, 167315,\n",
       "        215395,  90993, 206840, 210729,  22542])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randint(0,X.shape[0],(32,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c6db9b4b-9b08-49b4-b3cc-8c6c44a4e42b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set for updated NN\n",
    "\n",
    "# params\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C = torch.randn((27,2), generator=g)\n",
    "\n",
    "W1 = torch.randn((6,100), generator=g) # 6 inputs 100 outputs\n",
    "b1 = torch.randn(100, generator=g) # will be broadcasted when added to W1\n",
    "\n",
    "W2 = torch.randn((100,27), generator=g) # 100 outputs from hidden layer, 27 outputs (27 chars possible)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C,W1,b1,W2,b2]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d0aa760b-8608-4a5c-9c7a-78e4b6ee050b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TEST LEARNING RATE between values -1 and -0.001\n",
    "\n",
    "lre = torch.linspace(-3,0,1000) # we will get a 1000 values between -3 and 0 (which we will use as exponents)\n",
    "lrs = 10**lre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7c7b452d-703b-4b6b-9f93-28ba8da7ca29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TRAINING LOOP using minibatches\n",
    "# not as good quality as training on entire data set but it's way faster\n",
    "\n",
    "# collect learning rates as a list for diff values\n",
    "lri = []\n",
    "lossi = []\n",
    "\n",
    "for i in range(10000):\n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0,X.shape[0], (32,))\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[X[ix]] # UPDATED build LUT embedding matrix, (32,3,2)\n",
    "    h = torch.tanh(emb.view(-1,6) @ W1 + b1) # (32,100) hidden layer\n",
    "    logits = h @ W2 + b2 # (32,27)\n",
    "    loss = F.cross_entropy(logits,Y[ix]) # replaces counts, prob and loss lines above\n",
    "    #print(loss.item())\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None # initialize grad vals to 0\n",
    "    loss.backward() # populates gradients\n",
    "\n",
    "    # update parameters\n",
    "    #lr = lrs[i]\n",
    "    lr = 0.1\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad \n",
    "        \n",
    "    # track stats for learning rates\n",
    "    #lri.append(lre[i])\n",
    "    #lossi.append(loss.item())\n",
    "        \n",
    "#print(loss.item()) # calculates loss on minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "71633bfd-39bb-4ab4-8dff-6735c888c2b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# plot learning rates and losses\n",
    "#plt.plot(lri,lossi) # we see somewhere around 10^-1 i.e., -0.1 is a an ideal setting for learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "188d24c1-d4d1-4362-b5d6-c35715f60ac7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.4920, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate loss for entire data set after training\n",
    "emb = C[X] \n",
    "h = torch.tanh(emb.view(-1,6) @ W1 + b1) # hidden layer\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Y)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ea61ebb-a839-4bcd-af71-3f6f809fec1d",
   "metadata": {},
   "source": [
    "### It's better to have an approx gradient and make more steps, than to eval the exact gradient and take fewer steps. This is why minibatches work well in practice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5508852f-08eb-43fc-a25a-3f65f49ef27f",
   "metadata": {},
   "source": [
    "## How to find a good initial learning rate:\n",
    "\n",
    "Too big: \n",
    "- E.g.,  `p.data += -10 * p.grad` will lead to erratic behavior and cause the NN to not learn effectively. It'll jump up and down sharply in loss values.\n",
    "    \n",
    "Too small:\n",
    "- E.g., e.g., `p.data += -0.00001 * p.grad` will learn way too slowly (if at all)\n",
    "\n",
    "\n",
    "See TEST LEARNING RATE above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec9ee1f-02ef-4eae-88d6-f26b33dd3967",
   "metadata": {},
   "source": [
    "## Near the end of training it is standard to use a learning rate decay\n",
    "\n",
    "This involves lowering the magnitude of the learning rate so that smaller steps are taken near the end. Guessing this is for more precision?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a595f35e-5f02-486c-a1b5-0f48b8954370",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Avoiding overfitting to dataset (where NN just remembers dataset verbatim, having 0 loss on training set and high loss on novel data)\n",
    "\n",
    "### Splitting up dataset into 3 splits:\n",
    "1. Training split\n",
    "- Typically 80% of dataset\n",
    "- Used to optimize params of model using grad descent (like we did above)\n",
    "2. Dev/Validation split\n",
    "- Typically 10% of dataset\n",
    "- Used to train/test hyperparams (size of hidden layer, size of embedding matrix LUT, strength of regularization, etc.)\n",
    "3. Test split\n",
    "- Typically 10% of dataset\n",
    "- Test on test spli dataset sparingly to avoid training on it and overfitting to it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "61230df9-c1b8-4dc4-b467-d1db28f52527",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# build the dataset (split into 3)\n",
    "\n",
    "def build_dataset(words):\n",
    "    block_size = 3 # context length, how many chars we take to predict the next one\n",
    "    X, Y = [], []\n",
    "    for w in words:\n",
    "        context = [0] * block_size # padded tokens\n",
    "        for ch in w + '.':\n",
    "            ix = stoi[ch]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix] # crop and append\n",
    "            \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    #print(X.shape,Y.shape)\n",
    "    return X,Y\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "random.shuffle(words) # randomly shuffle words (so unsorted)\n",
    "n1 = int(0.8*len(words))\n",
    "n2 = int(0.9*len(words))\n",
    "\n",
    "Xtr, Ytr = build_dataset(words[:n1]) # build training dataset (words 80% of dataset)\n",
    "Xdev, Ydev = build_dataset(words[n1:n2]) # build dev dataset (words 80-90, so 10%)\n",
    "Xte, Yte = build_dataset(words[n2:]) # build test dataset (words 90-100, so last 10% of dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d225a80c-9e19-48f1-ae08-b82aaa8d2f4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set for updated NN\n",
    "\n",
    "# params\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C = torch.randn((27,2), generator=g)\n",
    "\n",
    "W1 = torch.randn((6,100), generator=g) # 6 inputs 100 outputs\n",
    "b1 = torch.randn(100, generator=g) # will be broadcasted when added to W1\n",
    "\n",
    "W2 = torch.randn((100,27), generator=g) # 100 outputs from hidden layer, 27 outputs (27 chars possible)\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C,W1,b1,W2,b2]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1fa6ecb0-fd17-4708-933f-f580022cae6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TRAINING LOOP using minibatches\n",
    "# not as good quality as training on entire data set but it's way faster\n",
    "\n",
    "for i in range(10000):\n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0,Xtr.shape[0], (32,))\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[Xtr[ix]] # UPDATED build LUT embedding matrix, (32,3,2)\n",
    "    h = torch.tanh(emb.view(-1,6) @ W1 + b1) # (32,100) hidden layer\n",
    "    logits = h @ W2 + b2 # (32,27)\n",
    "    loss = F.cross_entropy(logits,Ytr[ix]) # replaces counts, prob and loss lines above\n",
    "    #print(loss.item())\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None # initialize grad vals to 0\n",
    "    loss.backward() # populates gradients\n",
    "\n",
    "    # update parameters\n",
    "    #lr = lrs[i]\n",
    "    lr = 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad \n",
    "        \n",
    "#print(loss.item()) # calculates loss on minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "a217e2fd-024d-4de0-a574-e5f6800fff4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3549227714538574\n"
     ]
    }
   ],
   "source": [
    "print(loss.item()) # calculates loss on minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b9979fa2-7dfa-4cc6-8c94-a4f1b22c36e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3427, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate loss for training dataset\n",
    "emb = C[Xtr] # changed to using Xdev for evaluation\n",
    "h = torch.tanh(emb.view(-1,6) @ W1 + b1) # hidden layer\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Ytr)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "61a0ecd4-2ebd-4c51-a38b-28a120c4b1b6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.3427, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate loss for dev dataset\n",
    "emb = C[Xdev] # changed to using Xdev for evaluation\n",
    "h = torch.tanh(emb.view(-1,6) @ W1 + b1) # hidden layer\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Ydev)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a11ad3e-196d-4787-ae9d-016676ca4019",
   "metadata": {},
   "source": [
    "### We are \"underfitting\" because the training loss and dev loss are roughly equal. This means we can make performance improvements by scaling up the size of NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1506181-ff5b-4236-8e3a-cd631458f845",
   "metadata": {},
   "source": [
    "## Next steps: Experimenting with larger hidden layer\n",
    "\n",
    "We should expect a lower loss because we have a bigger model (which should \"underfit\" less)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5d2b5ba1-6a41-4ae3-8ad0-e1410f61ebfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set for updated NN, LARGER HIDDEN LAYER\n",
    "\n",
    "# params\n",
    "g = torch.Generator().manual_seed(2147483647) # for reproducibility\n",
    "C = torch.randn((27,2), generator=g)\n",
    "\n",
    "# UPDATED\n",
    "W1 = torch.randn((6,300), generator=g) # 6 inputs 300 outputs <------\n",
    "b1 = torch.randn(300, generator=g) # will be broadcasted when added to W1\n",
    "W2 = torch.randn((300,27), generator=g) # 100 outputs from hidden layer, 27 outputs (27 chars possible)\n",
    "\n",
    "b2 = torch.randn(27, generator=g)\n",
    "parameters = [C,W1,b1,W2,b2]\n",
    "\n",
    "for p in parameters:\n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f8458b66-fe13-458f-bc11-41b01835b552",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#sum(p.nelement() for p in parameters) # num of parameters has updated since we increased size of hidden layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "79b79dae-534b-44e6-b490-5711f442e305",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TRAINING LOOP using minibatches\n",
    "# not as good quality as training on entire data set but it's way faster\n",
    "\n",
    "# collect learning rates as a list for diff values\n",
    "lri = []\n",
    "lossi = []\n",
    "stepsi = [] # UDPATED, used to plot loss against step size\n",
    "\n",
    "for i in range(10000):\n",
    "    # minibatch construct\n",
    "    ix = torch.randint(0,X.shape[0], (32,))\n",
    "    \n",
    "    # forward pass\n",
    "    emb = C[X[ix]] # UPDATED build LUT embedding matrix, (32,3,2)\n",
    "    h = torch.tanh(emb.view(-1,6) @ W1 + b1) # (32,100) hidden layer\n",
    "    logits = h @ W2 + b2 # (32,27)\n",
    "    loss = F.cross_entropy(logits,Y[ix]) # replaces counts, prob and loss lines above\n",
    "    #print(loss.item())\n",
    "    \n",
    "    # backward pass\n",
    "    for p in parameters:\n",
    "        p.grad = None # initialize grad vals to 0\n",
    "    loss.backward() # populates gradients\n",
    "\n",
    "    # update parameters\n",
    "    #lr = lrs[i]\n",
    "    lr = 0.01\n",
    "    for p in parameters:\n",
    "        p.data += -lr * p.grad \n",
    "        \n",
    "    # track stats for learning rates\n",
    "    #lri.append(lre[i])\n",
    "    stepsi.append(i)\n",
    "    lossi.append(loss.item())\n",
    "        \n",
    "#print(loss.item()) # calculates loss on minibatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "9cb763cd-435f-4b3d-8297-3b61f1f5eb0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x156499010>]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(stepsi,lossi) # the thickness in plot is due to noise (from small batch size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "c7bc6a8b-4dd0-4d04-9314-c3ed0c63371e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2890, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate loss for training dataset\n",
    "emb = C[Xtr] # changed to using Xdev for evaluation\n",
    "h = torch.tanh(emb.view(-1,6) @ W1 + b1) # hidden layer\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Ytr)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "07364b1e-6971-44e1-80f0-48c384c4c8ba",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.2815, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate loss for dev dataset\n",
    "emb = C[Xdev] # changed to using Xdev for evaluation\n",
    "h = torch.tanh(emb.view(-1,6) @ W1 + b1) # hidden layer\n",
    "logits = h @ W2 + b2\n",
    "loss = F.cross_entropy(logits, Ydev)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745f7b2a-ce0d-4ab8-a814-8dc6612feef2",
   "metadata": {},
   "source": [
    "### Potential bottleneck of network (preventing from getting lower loss): Embedding matrix has too few dimensions. So the problem could be that we are cramming too many characters in a 2D vector, so NN can't use that space effectively\n",
    "\n",
    "So we will try to change the character embedding size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2fb4f2-41e6-4773-b51c-cfb4089aefe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot to visualize embedding matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
